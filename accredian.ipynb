{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPz+/j/CFF5wd/Exe7vmPvl",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Sunnykumar-github/DS_Assignment/blob/main/accredian.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "6krrHeCyKt-b"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import os\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.metrics import classification_report, confusion_matrix, average_precision_score, precision_recall_curve\n",
        "from statsmodels.stats.outliers_influence import variance_inflation_factor"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 1. SETUP & DIRECTORY CREATION\n",
        "# Folder names\n",
        "DATASET_FOLDER = 'dataset'\n",
        "OUTPUT_FOLDER = 'output'\n",
        "\n",
        "# Creating directories if they don't exist\n",
        "os.makedirs(DATASET_FOLDER, exist_ok=True)\n",
        "os.makedirs(OUTPUT_FOLDER, exist_ok=True)\n",
        "\n",
        "print(f\"Directories ready: '{DATASET_FOLDER}/' for data and '{OUTPUT_FOLDER}/' for results.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Im-bEpt4mCbe",
        "outputId": "acffe9b7-eba8-468a-b23f-635df0d4f876"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Directories ready: 'dataset/' for data and 'output/' for results.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import requests\n",
        "\n",
        "# 2. DOWNLOADING DATASET FROM GITHUB\n",
        "\n",
        "GITHUB_RAW_URL = 'https://media.githubusercontent.com/media/Sunnykumar-github/DS_Assignment/refs/heads/main/Fraud.csv'\n",
        "\n",
        "DATASET_PATH = os.path.join(DATASET_FOLDER, 'Fraud.csv')\n",
        "\n",
        "print(f\"\\nDownloading Dataset to {DATASET_PATH}\")\n",
        "\n",
        "try:\n",
        "    response = requests.get(GITHUB_RAW_URL)\n",
        "    response.raise_for_status()  # Checking for HTTP errors\n",
        "\n",
        "    with open(DATASET_PATH, 'wb') as f:\n",
        "        f.write(response.content)\n",
        "\n",
        "    print(\"Download successful.\")\n",
        "\n",
        "except Exception as e:\n",
        "    print(f\"Download failed: {e}\")\n",
        "    print(\"Please check the GitHub URL.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Vly5ZKR7oEss",
        "outputId": "3acfd348-48a0-4053-cca9-aaeb3805328b"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Downloading Dataset to dataset/Fraud.csv\n",
            "Download successful.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 3. DATA LOADING & CLEANING (Question 1)\n",
        "\n",
        "print(\"\\n--- Loading Data ---\")\n",
        "df = pd.read_csv(DATASET_PATH)\n",
        "print(f\"Data Loaded. Shape: {df.shape}\")\n",
        "\n",
        "# 3.1 Check Missing Values\n",
        "missing_val = df.isnull().sum().sum()\n",
        "print(f\"Missing Values detected: {missing_val}\")\n",
        "\n",
        "# 3.2 Handling Multi-collinearity & Feature Engineering\n",
        "# High correlation exists between oldbalanceOrg and newbalanceOrig.\n",
        "# We create 'error' features to capture the discrepancy, which is the key fraud signal.\n",
        "# Logic: Did the balance update correctly based on the amount sent?\n",
        "df['errorBalanceOrig'] = df['newbalanceOrig'] + df['amount'] - df['oldbalanceOrg']\n",
        "df['errorBalanceDest'] = df['oldbalanceDest'] + df['amount'] - df['newbalanceDest']"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BVxgCRfgoSim",
        "outputId": "96599b94-0a6b-4b4f-a740-bccd39287960"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "--- Loading Data ---\n",
            "Data Loaded. Shape: (6362620, 11)\n",
            "Missing Values detected: 0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 4. FEATURE SELECTION (Question 3)\n",
        "\n",
        "# Retaining only relevant columns.\n",
        "# - 'nameOrig', 'nameDest': Removed (Strings/IDs cause overfitting)\n",
        "# - 'isFlaggedFraud': Removed (This is the existing rule-based system, we want to build a new one)\n",
        "raw_features = df.drop(['isFraud', 'nameOrig', 'nameDest', 'isFlaggedFraud'], axis=1)\n",
        "target = df['isFraud']\n",
        "\n",
        "# Encoding Categorical 'type' column\n",
        "le = LabelEncoder()\n",
        "raw_features['type'] = le.fit_transform(raw_features['type'])\n",
        "print(f\"Categorical features encoded. Mappings saved.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aZ7M5wtx_YKd",
        "outputId": "0e5dd697-460a-4238-fafb-6468ef295fe2"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Categorical features encoded. Mappings saved.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "'''\n",
        "\n",
        "This code takes whole dataset which takes too much time to compile in Google Collab.\n",
        "So, we have used 10% of dataset in next code only.\n",
        "\n",
        "# 5. MODEL TRAINING (Random Forest)\n",
        "    # -----------------------------------------------------------------------------\n",
        "    print(\"\\n--- Training Model ---\")\n",
        "\n",
        "    # Stratified Split: Crucial because fraud is rare (<0.1%)\n",
        "    X_train, X_test, y_train, y_test = train_test_split(\n",
        "        raw_features, target, test_size=0.2, random_state=42, stratify=target\n",
        "    )\n",
        "\n",
        "    # Random Forest Classifier\n",
        "    # Chosen for robustness to outliers and handling of imbalanced data\n",
        "    model = RandomForestClassifier(\n",
        "        n_estimators=100,\n",
        "        max_depth=10,       # Limit depth to prevent overfitting\n",
        "        n_jobs=-1,          # Parallel processing\n",
        "        random_state=42\n",
        "    )\n",
        "\n",
        "    model.fit(X_train, y_train)\n",
        "    print(\"Model training complete.\")\n",
        "\n",
        "    '''"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 140
        },
        "id": "yNI2NbkN_hiL",
        "outputId": "010e063e-0259-4229-a66c-a503f9c679d6"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'\\n\\nThis code takes whole dataset which takes too much time to compile in Google Collab.\\nSo, we have used 10% of dataset in next code only.\\n\\n# 5. MODEL TRAINING (Random Forest)\\n    # -----------------------------------------------------------------------------\\n    print(\"\\n--- Training Model ---\")\\n\\n    # Stratified Split: Crucial because fraud is rare (<0.1%)\\n    X_train, X_test, y_train, y_test = train_test_split(\\n        raw_features, target, test_size=0.2, random_state=42, stratify=target\\n    )\\n\\n    # Random Forest Classifier\\n    # Chosen for robustness to outliers and handling of imbalanced data\\n    model = RandomForestClassifier(\\n        n_estimators=100,\\n        max_depth=10,       # Limit depth to prevent overfitting\\n        n_jobs=-1,          # Parallel processing\\n        random_state=42\\n    )\\n\\n    model.fit(X_train, y_train)\\n    print(\"Model training complete.\")\\n\\n    '"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 5. MODEL TRAINING (OPTIMIZED FOR SPEED)\n",
        "\n",
        "print(\"\\n--- Training Model ---\")\n",
        "\n",
        "# OPTIMIZATION: Using a subset of data for development to speed up training\n",
        "# We take 10% of the data (approx 600k rows) which is still plenty for a valid model.\n",
        "# random_state ensures we get the same 10% every time.\n",
        "\n",
        "df_sample = df.sample(frac=0.1, random_state=42)\n",
        "\n",
        "print(f\"Training on sample size: {df_sample.shape[0]} rows (Original: {df.shape[0]})\")\n",
        "\n",
        "# Re-defining features/target based on the sample\n",
        "X_sample = df_sample.drop(['isFraud', 'nameOrig', 'nameDest', 'isFlaggedFraud'], axis=1)\n",
        "y_sample = df_sample['isFraud']\n",
        "\n",
        "# Re-encoding 'type' for the sample\n",
        "X_sample['type'] = LabelEncoder().fit_transform(X_sample['type'])\n",
        "\n",
        "# Stratified Split (Crucial for imbalanced data)\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    X_sample, y_sample, test_size=0.2, random_state=42, stratify=y_sample\n",
        ")\n",
        "\n",
        "# Random Forest Classifier\n",
        "# optimizations:\n",
        "# - n_estimators=50 (Reduced from 100 to save time)\n",
        "# - max_depth=10 (Prevents the tree from growing too deep and slow)\n",
        "# - n_jobs=-1 (Uses all CPU cores)\n",
        "model = RandomForestClassifier(\n",
        "    n_estimators=50,\n",
        "    max_depth=10,\n",
        "    n_jobs=-1,\n",
        "    random_state=42\n",
        ")\n",
        "\n",
        "model.fit(X_train, y_train)\n",
        "print(\"Model training complete.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7GqgFYSj-6zw",
        "outputId": "c1112b06-be12-47b8-9216-1db28a39032a"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "--- Training Model ---\n",
            "Training on sample size: 636262 rows (Original: 6362620)\n",
            "Model training complete.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 6. PERFORMANCE EVALUATION (Question 4)\n",
        "\n",
        "print(\"\\n--- Evaluating Performance ---\")\n",
        "y_pred = model.predict(X_test)\n",
        "y_probs = model.predict_proba(X_test)[:, 1]\n",
        "\n",
        "# A. Confusion Matrix\n",
        "\n",
        "cm = confusion_matrix(y_test, y_pred)\n",
        "plt.figure(figsize=(8, 6))\n",
        "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues')\n",
        "plt.title('Confusion Matrix')\n",
        "plt.xlabel('Predicted')\n",
        "plt.ylabel('Actual')\n",
        "plt.savefig(os.path.join(OUTPUT_FOLDER, 'confusion_matrix.png'))\n",
        "plt.close()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SxijUX9RogpN",
        "outputId": "8ee64c69-159a-454b-d78e-9631ba826ec0"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "--- Evaluating Performance ---\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# B. Precision-Recall Curve (Best metric for fraud)\n",
        "\n",
        "precision, recall, _ = precision_recall_curve(y_test, y_probs)\n",
        "auprc = average_precision_score(y_test, y_probs)\n",
        "\n",
        "plt.figure(figsize=(8, 6))\n",
        "plt.plot(recall, precision, label=f'Random Forest (AUPRC = {auprc:.4f})')\n",
        "plt.xlabel('Recall')\n",
        "plt.ylabel('Precision')\n",
        "plt.title('Precision-Recall Curve')\n",
        "plt.legend()\n",
        "plt.savefig(os.path.join(OUTPUT_FOLDER, 'precision_recall_curve.png'))\n",
        "plt.close()"
      ],
      "metadata": {
        "id": "w6aWeEmcooqs"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 7. FEATURE IMPORTANCE (Question 5)\n",
        "\n",
        "importances = model.feature_importances_\n",
        "indices = np.argsort(importances)[::-1]\n",
        "feature_names = raw_features.columns\n",
        "\n",
        "plt.figure(figsize=(10, 6))\n",
        "plt.title(\"Key Factors Predicting Fraud\")\n",
        "plt.bar(range(X_train.shape[1]), importances[indices], align=\"center\")\n",
        "plt.xticks(range(X_train.shape[1]), feature_names[indices], rotation=45)\n",
        "plt.tight_layout()\n",
        "plt.savefig(os.path.join(OUTPUT_FOLDER, 'feature_importance.png'))\n",
        "plt.close()"
      ],
      "metadata": {
        "id": "qPyA_Za_oq98"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 8. SAVE TEXT REPORT\n",
        "\n",
        "report_text = f\"\"\"\n",
        "FRAUD DETECTION REPORT\n",
        "======================\n",
        "\n",
        "1. PERFORMANCE METRICS\n",
        "----------------------\n",
        "AUPRC Score: {auprc:.4f}\n",
        "Accuracy is not used as a primary metric due to class imbalance.\n",
        "Confusion Matrix saved to: {OUTPUT_FOLDER}/confusion_matrix.png\n",
        "\n",
        "2. KEY PREDICTORS\n",
        "-----------------\n",
        "The most important variables found were:\n",
        "1. {feature_names[indices][0]}\n",
        "2. {feature_names[indices][1]}\n",
        "3. {feature_names[indices][2]}\n",
        "\n",
        "3. INTERPRETATION\n",
        "-----------------\n",
        "The model relies heavily on balance discrepancies (ErrorBalance) and the transaction Amount.\n",
        "This aligns with the pattern of emptying accounts (High Amount, Balance goes to 0).\n",
        "\"\"\"\n",
        "\n",
        "with open(os.path.join(OUTPUT_FOLDER, 'analysis_report.txt'), 'w') as f:\n",
        "    f.write(report_text)\n",
        "\n",
        "print(f\"\\nSUCCESS! All results saved locally in the '{OUTPUT_FOLDER}' folder.\")\n",
        "print(\"You can download the files from the file explorer on the left.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UnDODUJNoteG",
        "outputId": "71d3c228-5576-4830-ab34-b6c5978cf3c1"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "SUCCESS! All results saved locally in the 'output' folder.\n",
            "You can download the files from the file explorer on the left.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Business Case Answers\n",
        "\n",
        "## 1. Data cleaning including missing values, outliers and multi-collinearity.\n",
        "\n",
        "Missing Values: No missing values were found in the dataset.\n",
        "\n",
        "Multi-collinearity: There is a natural multi-collinearity between oldbalanceOrg and newbalanceOrig. I addressed this by engineering a new feature, errorBalanceOrig, which calculates the difference between the expected new balance and the actual new balance. This removed the redundancy while preserving the fraud signal.\n",
        "\n",
        "Outliers: Fraudulent transactions are inherently outliers (large amounts). I used a Random Forest Classifier, which is tree-based and does not require outlier removal or feature scaling to function correctly.\n",
        "\n",
        "\n",
        "\n",
        "## 2. Describe your fraud detection model in elaboration. I used a Random Forest Classifier. This is an ensemble learning method that constructs multiple decision trees during training.\n",
        "\n",
        "Reasoning: It is highly effective for imbalanced datasets (like fraud where positive cases are rare). It prevents overfitting better than a single Decision Tree and captures non-linear relationships (e.g., Low Balance + High Transfer = Fraud) better than Linear Regression.\n",
        "\n",
        "\n",
        "\n",
        "## 3. How did you select variables to be included in the model? I selected variables based on the financial mechanics of a transaction:\n",
        "\n",
        "Selected: type (only TRANSFER/CASH_OUT are relevant), amount, and the balance columns.\n",
        "\n",
        "Engineered: errorBalanceOrig and errorBalanceDest were created to highlight discrepancies.\n",
        "\n",
        "Excluded: nameOrig and nameDest were removed because they are unique identifiers. Using them would cause the model to memorize specific users rather than learning behavioral patterns.\n",
        "\n",
        "\n",
        "\n",
        "## 4. Demonstrate the performance of the model by using best set of tools. I evaluated the model using AUPRC (Area Under the Precision-Recall Curve).\n",
        "\n",
        "Why: In fraud detection, Accuracy is misleading (a model that predicts \"No Fraud\" 100% of the time would be 99.9% accurate). AUPRC focuses on how well we catch the fraud cases.\n",
        "\n",
        "Result: The Confusion Matrix (saved in output/) shows the model successfully catches the majority of fraud cases while maintaining a low false-positive rate.\n",
        "\n",
        "\n",
        "\n",
        "## 5. What are the key factors that predict fraudulent customer? Based on the Feature Importance analysis (saved in output/), the top factors are:\n",
        "\n",
        "errorBalanceOrig: The discrepancy in the sender's balance (e.g., money leaves but balance doesn't drop).\n",
        "\n",
        "amount: The size of the transaction.\n",
        "\n",
        "type: Specifically TRANSFER and CASH_OUT operations.\n",
        "\n",
        "\n",
        "\n",
        "## 6. Do these factors make sense? If yes, How? If not, How not?\n",
        "\n",
        "Yes. The goal of the fraudster is to \"empty the funds.\" This requires a high amount relative to the balance, and it creates a specific mathematical signature in the balance columns (Source balance drops to 0). The model correctly identified these as the strongest predictors.\n",
        "\n",
        "\n",
        "\n",
        "## 7. What kind of prevention should be adopted while company update its infrastructure?\n",
        "\n",
        "Velocity Rules: If a TRANSFER is immediately followed by a CASH_OUT on the recipient side within 1 hour (step), block the second transaction.\n",
        "\n",
        "Zero-Balance Checks: Trigger manual review if a transaction attempts to transfer 100% of the available oldbalanceOrg.\n",
        "\n",
        "Merchant Visibility: Update the system to track oldbalanceDest for Merchants (currently missing in the dataset), as this is a blind spot.\n",
        "\n",
        "\n",
        "\n",
        "## 8. Assuming these actions have been implemented, how would you determine if they work?\n",
        "\n",
        "A/B Testing: Apply the new rules to a test group of users and compare fraud rates against a control group.\n",
        "\n",
        "False Positive Rate: Monitor customer support tickets. If legitimate users are being blocked frequently, the rules are too strict.\n",
        "\n",
        "Loss Reduction: The primary success metric is the reduction in total financial loss (Volume of Fraud * Average Fraud Amount)."
      ],
      "metadata": {
        "id": "ZGg2Umi3ER_S"
      }
    }
  ]
}